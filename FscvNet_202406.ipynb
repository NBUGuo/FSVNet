{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import copy \n",
    "import os\n",
    "import openpyxl\n",
    "from openpyxl import Workbook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image   #  pip install pillow\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PyTorch version:', torch.__version__) \n",
    "print('Python version:',sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_path = glob.glob(r'Data/*/*.tif')\n",
    "imgs_path[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_path[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_p = imgs_path[7]\n",
    "img_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_p.split('\\\\')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [img_p.split('\\\\')[1] for img_p in imgs_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_label = np.unique(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_index = dict((v, k) for k, v in enumerate(unique_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_label = dict((v, k) for k, v in label_to_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = [label_to_index.get(la) for la in label_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels[-5: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(imgs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2023)\n",
    "random_index = np.random.permutation(len(imgs_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_path = np.array(imgs_path)[random_index]\n",
    "all_labels = np.array(all_labels)[random_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_path[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_path[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data is divided into 60%, 30%, and 10% of the training set, test set, and validation set, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = int(len(imgs_path)*0.6)\n",
    "eval_len = int(len(imgs_path)*0.2)\n",
    "\n",
    "train_path = imgs_path[ :train_len]\n",
    "train_labels = all_labels[ :train_len]\n",
    "\n",
    "eval_path = imgs_path[train_len: train_len+eval_len]\n",
    "eval_labels = all_labels[train_len:train_len+eval_len]\n",
    "\n",
    "test_path = imgs_path[train_len+eval_len:]\n",
    "test_labels = all_labels[train_len+eval_len:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([                   \n",
    "                    transforms.Resize((1024, 1024)),\n",
    "                    #transforms.CenterCrop((680,680)),\n",
    "                    #transforms.RandomRotation(degrees=(40)),  \n",
    "                    transforms.ToTensor(),   \n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])# mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5]\n",
    "])\n",
    "\n",
    "eval_transform = transforms.Compose([\n",
    "                    transforms.Resize((1024, 1024)),\n",
    "                    #transforms.CenterCrop((680,680)),\n",
    "                    #transforms.RandomRotation(degrees=(40)),   \n",
    "                    transforms.ToTensor(), \n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])# mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5]\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "                    transforms.Resize((1024, 1024)),\n",
    "                    #transforms.CenterCrop((680,680)),\n",
    "                    #transforms.RandomRotation(degrees=(40)),   \n",
    "                    transforms.ToTensor(), \n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])# mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a subclass of the Dataset\n",
    "class Mydataset(data.Dataset):       \n",
    "    def __init__(self, img_paths, labels,transform):  #Initialize the path to the image\n",
    "        self.imgs = img_paths\n",
    "        self.labels = labels\n",
    "        self.transforms = transform\n",
    "\n",
    "    def __getitem__(self, index):   #Implement data slicing\n",
    "        img = self.imgs[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        #pil_img = Image.open(img)    \n",
    "        pil_img = glob.glob(img)\n",
    "        pil_img=Image.fromarray(pil_img)\n",
    "        \n",
    "        data = self.transforms(pil_img)\n",
    "\n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):            #Returns the total length of the data\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Mydataset(train_path, train_labels, train_transform)\n",
    "eval_ds = Mydataset(eval_path, eval_labels, eval_transform)\n",
    "test_ds = Mydataset(test_path, test_labels, test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE =10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = data.DataLoader(\n",
    "                           train_ds,\n",
    "                           batch_size=BATCH_SIZE,\n",
    "                           shuffle=True\n",
    "                           )\n",
    "\n",
    "eval_dl = data.DataLoader(\n",
    "                          eval_ds,\n",
    "                          batch_size=BATCH_SIZE\n",
    "                          )\n",
    "\n",
    "test_dl = data.DataLoader(\n",
    "                          test_ds,\n",
    "                          batch_size=BATCH_SIZE\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_batch, labels_batch = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "for i, (img, label) in enumerate(zip(imgs_batch[-6:], labels_batch[-6:])):\n",
    "    #img = img.permute(1, 2, 0).numpy()\n",
    "    img = (img.permute(1, 2, 0).numpy()+1)/2    \n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.title(index_to_label.get(label.item()))\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im=imgs_batch[0].permute(1,2,0)\n",
    "im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#im=im.numpy()\n",
    "img = (im+1)/2   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct FSVNet model - use CNN to extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 16, 3)\n",
    "        self.bn1=nn.BatchNorm2d(16)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 32, 3)\n",
    "        self.bn2=nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, 3)\n",
    "        self.bn3=nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(64, 128, 3)\n",
    "        self.bn4=nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(128,256,3)\n",
    "        self.bn5=nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(256,512,3)\n",
    "        self.bn6=nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv7 = nn.Conv2d(512,1024,3)\n",
    "        self.bn7=nn.BatchNorm2d(1024)\n",
    "\n",
    "        self.conv8 = nn.Conv2d(1024,2048,3)\n",
    "        self.bn8=nn.BatchNorm2d(2048)\n",
    "    \n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(2048*2*2, 4096)   \n",
    "        self.bn_f1=nn.BatchNorm1d(4096)  \n",
    "        self.fc2 = nn.Linear(4096, 2048)\n",
    "        self.bn_f2=nn.BatchNorm1d(2048) \n",
    "        self.fc3 = nn.Linear(2048, 1024) \n",
    "        self.bn_f3=nn.BatchNorm1d(1024) \n",
    "        self.fc4 = nn.Linear(1024,512)  \n",
    "        self.bn_f4=nn.BatchNorm1d(512) \n",
    "        self.fc5 = nn.Linear(512, 256) \n",
    "        self.bn_f5=nn.BatchNorm1d(256) \n",
    "        self.fc6 = nn.Linear(256, 128)  \n",
    "        self.bn_f6=nn.BatchNorm1d(128) \n",
    "        self.fc7= nn.Linear(128, 2)  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.pool(F.relu(self.conv1(x)))\n",
    "        x=self.bn1(x)\n",
    "        x=self.pool(F.relu(self.conv2(x)))\n",
    "        x=self.bn2(x)\n",
    "        x=self.pool(F.relu(self.conv3(x)))\n",
    "        x=self.bn3(x)\n",
    "        x=self.pool(F.relu(self.conv4(x)))\n",
    "        x=self.bn4(x)  \n",
    "        x=self.pool(F.relu(self.conv5(x)))\n",
    "        x=self.bn5(x)\n",
    "        x=self.pool(F.relu(self.conv6(x)))\n",
    "        x=self.bn6(x)  \n",
    "        x=self.pool(F.relu(self.conv7(x))) \n",
    "        x=self.bn7(x) \n",
    "        x=self.pool(F.relu(self.conv8(x))) \n",
    "        x=self.bn8(x)        \n",
    "        #x=self.pool(F.relu(self.conv9(x))) \n",
    "        #x=self.bn9(x) \n",
    "        #x=self.pool(F.relu(self.conv10(x))) \n",
    "        #x=self.bn10(x)         \n",
    "        x=self.drop(x)        \n",
    "        #print(x.size())\n",
    "        x=x.view(-1, 2048*2*2)        \n",
    "        #x=x.view(-1, x.size(1)*x.size(2)*x.size(3))       \n",
    "        #print(x.size())\n",
    "        x=F.relu(self.fc1(x))\n",
    "        #print(x.size())\n",
    "        \n",
    "        x=self.bn_f1(x)        \n",
    "        x=self.drop(x) \n",
    "        \n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.bn_f2(x)\n",
    "        x=self.drop(x) \n",
    "        \n",
    "        x=F.relu(self.fc3(x))\n",
    "        x=self.bn_f3(x)\n",
    "        x=self.drop(x)\n",
    "        \n",
    "        x=F.relu(self.fc4(x))\n",
    "        x=self.bn_f4(x)\n",
    "        x=self.drop(x)\n",
    "\n",
    "        x=F.relu(self.fc5(x))\n",
    "        x=self.bn_f5(x)\n",
    "        x=self.drop(x)\n",
    "\n",
    "        x=F.relu(self.fc6(x))\n",
    "        x=self.bn_f6(x)\n",
    "        x=self.drop(x)\n",
    "\n",
    "        x=F.relu(self.fc7(x))\n",
    "       \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model=Net()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The amount of model computation and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1：\n",
    "from torchsummary import summary\n",
    "\n",
    "#print(summary(model, input_size=(3, 1024, 1024)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2：\n",
    "#total = sum([param.nelement() for param in model.parameters()]) \n",
    "#print(\"Number of parameter: %.2fM\" % (total/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "from thop import profile\n",
    "\n",
    "#input = torch.randn(1, 3,1024,1024)\n",
    "#flops, params = profile(model, inputs=(input,))\n",
    "#print('flops: ', flops, 'params: ', params)\n",
    "#print(' flops: %.2f M, params: %.2f M' % (flops //1e6, params //1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model(imgs_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training function\n",
    "def train_model(model, train_dataloader, criterion, optimizer, device):\n",
    "\n",
    "    correct=0\n",
    "    total=0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    start_time = time.time()    \n",
    "\n",
    "    model.train()\n",
    "    for inputs, labels in train_dataloader:\n",
    "        labels=torch.as_tensor(labels,dtype=torch.long)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)        \n",
    "\n",
    "        predict = model(inputs)\n",
    "        loss = criterion(predict,labels) \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            predict=torch.argmax(predict,dim=1)\n",
    "            correct+=(predict==labels).sum().item()\n",
    "            total+=labels.size(0)\n",
    "            running_loss+=loss.item()\n",
    "\n",
    "    loss = running_loss / len(train_dataloader.dataset)\n",
    "    acc = correct / total\n",
    "\n",
    "    train_time = time.time() - start_time \n",
    "\n",
    "    return loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validate model functions\n",
    "def eval_model(model, eval_dataloader,criterion, device):\n",
    "\n",
    "    eval_correct = 0\n",
    "    eval_total = 0\n",
    "    eval_running_loss = 0.0\n",
    "\n",
    "    predictions = []\n",
    "    targets = []\n",
    "\n",
    "    start_time = time.time()    \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in eval_dataloader:\n",
    "            labels=torch.as_tensor(labels,dtype=torch.long)\n",
    "            inputs,labels = inputs.to(device),labels.to(device)\n",
    "\n",
    "            predict = model(inputs)\n",
    "            loss = criterion(predict, labels) \n",
    "\n",
    "            predict=torch.argmax(predict,dim=1)\n",
    "\n",
    "            eval_correct+=(predict==labels).sum().item()\n",
    "            eval_total+=labels.size(0)\n",
    "            eval_running_loss+=loss.item()\n",
    "\n",
    "            predictions.extend(predict.cpu().numpy())\n",
    "            targets.extend(labels.cpu().numpy())\n",
    "\n",
    "    loss = eval_running_loss / len(eval_dataloader.dataset)\n",
    "    acc = eval_correct / eval_total\n",
    "\n",
    "    eval_time = time.time() - start_time \n",
    "\n",
    "    return loss, acc, predictions, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model function\n",
    "def test_model(model, test_loader,device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "    save_dir='DataReport'\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "\n",
    "    book = openpyxl.Workbook()\n",
    "    sheet = book.active\n",
    "    sheet.title = \"DataReport\"\n",
    "\n",
    "    sheet.append(['ground truth', 'preds'])\n",
    "\n",
    "    for i in range(0, len(all_preds)):\n",
    "        sheet.append([all_labels[i],all_preds[i]])\n",
    "\n",
    "    book.save(save_dir+\"/FsvNet_labels_preds.xlsx\")\n",
    "\n",
    "    return  all_labels,all_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeat the training rounds\n",
    "epochs = 20\n",
    "\n",
    "# Define the loss function and optimizer  \n",
    "criterion = torch.nn.CrossEntropyLoss()   #criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "#Model data logging\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_acc = 0.0\n",
    "\n",
    "#Training data logging\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "\n",
    "#Evaluate data logging\n",
    "test_preds = []\n",
    "test_labels = []  \n",
    "\n",
    "# Training and evaluation\n",
    "start_time = time.time()    \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    #model train\n",
    "    epoch_train_loss,epoch_train_acc = train_model(model, train_dl, criterion, optimizer, device)           \n",
    "\n",
    "    # test and evaluate\n",
    "    epoch_eval_loss,epoch_eval_acc,eval_predictions, eval_targets = eval_model(model, eval_dl,criterion, device)\n",
    "\n",
    "    #loss and acc. output\n",
    "    print(f\"Epoch {epoch+1}/{epochs}: Train Loss: {epoch_train_loss:.4f}   Train Acc: {epoch_train_acc:.4f}  Eval_Loss：{epoch_eval_loss:.4f}   Eval_Acc:{epoch_eval_acc:.4f}\")\n",
    "\n",
    "    if epoch_eval_acc>best_acc:\n",
    "        best_acc=epoch_eval_acc\n",
    "        best_model_wts=copy.deepcopy(model.state_dict())\n",
    "\n",
    "    train_loss.append(epoch_train_loss)\n",
    "    train_acc.append(epoch_train_acc)\n",
    "    test_loss.append(epoch_eval_loss)\n",
    "    test_acc.append(epoch_eval_acc)        \n",
    "\n",
    "\n",
    "model.load_state_dict(best_model_wts) \n",
    "\n",
    "duration = (time.time() - start_time)/60\n",
    "\n",
    "print(f\"Model_best_acc: {best_acc:.4f} ,Lasted {duration:.0f}min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model parameters are saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model weights to a file\n",
    "PATH='FscvNet_Weights.pth'\n",
    "torch.save(model.state_dict(),PATH)\n",
    "print(\"Model weights saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss plotting\n",
    "plt.plot(range(1, epochs+1), train_loss, label='train_loss')\n",
    "plt.plot(range(1, epochs+1), test_loss, label='test_loss')\n",
    "plt.legend()\n",
    "\n",
    "# save\n",
    "save_dir='DataReport'\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "plt.savefig(save_dir+'BaseCnn_loss.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#acc. plotting\n",
    "plt.plot(range(1, epochs+1), train_acc, label='train_acc')\n",
    "plt.plot(range(1, epochs+1), test_acc, label='test_acc')\n",
    "plt.legend()\n",
    "\n",
    "# save\n",
    "save_dir='DataReport'\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "plt.savefig(save_dir+'BaseCnn_Acc.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss, accuracy data storage\n",
    "save_dir='DataReport'\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "book = openpyxl.Workbook()\n",
    "sheet = book.active\n",
    "sheet.title = \"DataReport\"\n",
    "\n",
    "sheet.append(['epochs', 'train_loss','test_loss','train_acc','test_acc'])\n",
    "\n",
    "for i in range(0, epochs):\n",
    "    sheet.append([i+1, train_loss[i], test_loss[i],train_acc[i],test_acc[i]])\n",
    "\n",
    "book.save(save_dir+\"/BaseCnn_Loss_Accuracy.xlsx\")\n",
    "\n",
    "\n",
    "#range(0,epochs).to_excel(save_dir+\"/ResNet_Loss_Accuracy.xlsx\", encoding=\"utf_8_sig\")\n",
    "#train_loss.to_excel(save_dir+\"/ResNet_Loss_Accuracy.xlsx\", encoding=\"utf_8_sig\")\n",
    "#test_loss.to_excel(save_dir+\"/ResNet_Loss_Accuracy.xlsx\", encoding=\"utf_8_sig\")\n",
    "#test_loss.to_excel(save_dir+\"/ResNet_Loss_Accuracy.xlsx\", encoding=\"utf_8_sig\")\n",
    "#test_acc.to_excel(save_dir+\"/ResNet_Loss_Accuracy.xlsx\", encoding=\"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model parameter import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Net()\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "print(\"Model weights reloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "labels,preds=test_model(model, test_dl, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(labels, preds)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# cm = confusion_matrix(y_true=y_test, y_pred=y_pred, normalize='true')\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm,labels,preds): \n",
    "\n",
    "    # confusion matrix\n",
    "    class_names = ['signal1','signal2']\n",
    "    title=\"confusion_matrix\"\n",
    "    ylabel='Ground Truth'\n",
    "    xlabel='Predicted Label'\n",
    "\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "\n",
    "    #plt.figure(figsize=(18, 12))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.BuPu_r)  \n",
    "    #plt.imshow(cm, interpolation='nearest',cmap='RdBu')\n",
    "    #plt.colorbar(label='tota1 test sample number')    \n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xticks(tick_marks,class_names,rotation=45)\n",
    "    plt.yticks(tick_marks,class_names)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlabel(xlabel)\n",
    "\n",
    "     # save\n",
    "    save_dir='DataReport'\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "\n",
    "    plt.savefig(save_dir+'BaseCnn_Confusion_Matrix.png')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm,labels,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm,labels,preds): \n",
    "\n",
    "    # plot\n",
    "    class_names = ['signal1','signal2']\n",
    "    title=\"confusion_matrix\"\n",
    "    ylabel='Ground Truth'\n",
    "    xlabel='Predicted Label'\n",
    "\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "\n",
    "    #plt.figure(figsize=(18, 12))\n",
    "    #plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)  \n",
    "    plt.imshow(cm, interpolation='nearest',cmap='BuPu')\n",
    "    #plt.colorbar(label='tota1 test sample number')    \n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xticks(tick_marks,class_names,rotation=45)\n",
    "    plt.yticks(tick_marks,class_names)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlabel(xlabel)\n",
    "\n",
    "     # save\n",
    "    save_dir='DataReport'\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "\n",
    "    plt.savefig(save_dir+'BaseCnn_Confusion_Matrix.png')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm,labels,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(labels,preds)\n",
    "class_names = ['signal1','signal2']\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "#disp.plot() \n",
    "disp.plot(cmap=plt.cm.Blues_r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(labels,preds)\n",
    "class_names = ['signal1','signal2']\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "disp.plot(cmap=plt.cm.BuPu)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "cm = confusion_matrix(labels,preds)\n",
    "class_names = ['signal1','signal2']\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "disp.plot(cmap=plt.cm.PuBu)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "cm = confusion_matrix(labels,preds)\n",
    "class_names = ['signal1','signal2']\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "disp.plot(cmap=plt.cm.PuBu_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "class_names = ['signal1','signal2']\n",
    "title=\"confusion_matrix\"\n",
    "ylabel='Ground Truth'\n",
    "xlabel='Predicted Label'\n",
    "\n",
    "tick_marks = np.arange(len(class_names))\n",
    "\n",
    "cm=confusion_matrix(labels, preds)\n",
    "\n",
    "#sns.heatmap(cm, annot=True) \n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False) \n",
    "#sns.heatmap(cm, annot=True,cmap='PuOr') \n",
    "#sns.heatmap(cm, cmap=\"YlGnBu\",annot=True)\n",
    "\n",
    "plt.xlabel(xlabel,fontsize=20, color='k') \n",
    "plt.ylabel(ylabel,fontsize=20, color='k') \n",
    "plt.xticks(tick_marks,class_names,rotation=45) \n",
    "plt.yticks(tick_marks,class_names)  \n",
    "plt.title(title,fontsize=20) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "class_names = ['signal1','signal2']\n",
    "title=\"confusion_matrix\"\n",
    "ylabel='Ground Truth'\n",
    "xlabel='Predicted Label'\n",
    "\n",
    "tick_marks = np.arange(len(class_names))\n",
    "\n",
    "cm=confusion_matrix(labels, preds)\n",
    "\n",
    "#sns.heatmap(cm, annot=True) \n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False) \n",
    "#sns.heatmap(cm, annot=True,cmap='PuOr') \n",
    "#sns.heatmap(cm, cmap=\"YlGnBu\",annot=True)\n",
    "\n",
    "plt.xlabel(xlabel,fontsize=20, color='k') \n",
    "plt.ylabel(ylabel,fontsize=20, color='k') \n",
    "plt.xticks(tick_marks,class_names,rotation=45) \n",
    "plt.yticks(tick_marks,class_names)  \n",
    "plt.title(title,fontsize=20) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive evaluation of model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics  \n",
    "from sklearn.metrics import f1_score, mean_absolute_error, mean_squared_error, roc_auc_score  \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, ndcg_score \n",
    " \n",
    "# Define the function for calculating F1-score \n",
    "def calculate_f1_score(true_labels, predicted_labels): \n",
    " return f1_score(true_labels, predicted_labels) \n",
    " \n",
    "# Define the function for calculating MAE \n",
    "def calculate_mae(true_ values, predicted_values): \n",
    " return mean_absolute_error(true_values, predicted_values) \n",
    " \n",
    "# Define the function to calculate RMSE \n",
    "def calculate_rmse(true_values, predicted_values): \n",
    " return np.sqrt(mean_ squared_error(true_values, predicted_values)) \n",
    " \n",
    " \n",
    "# Define the function for calculating Accuracy \n",
    "def calculate_accuracy(true_labels, predicted_labels): \n",
    " return accuracy_score(true_labels, predicted_labels \n",
    " \n",
    "# Calculate Precision@k metrics \n",
    "def calculate_precision(true_labels, predicted_labels):  \n",
    " return precision_score(true_labels, predicted_labels) \n",
    " \n",
    "# Define the function to calculate Recall \n",
    "def calculate_recall(true_ labels, predicted_labels): \n",
    " return recall_score(true_labels, predicted_labels) \n",
    " \n",
    " \n",
    "# Define the function for calculating ROC AUC  \n",
    "def calculate_roc_auc(true_labels, predicted_scores): \n",
    " return roc_auc_score( true_labels, predicted_scores) \n",
    " \n",
    "# Calculate NDCG@k metrics  \n",
    "def compute_ndcg(predictions, targets, k):  \n",
    " ndcg_scores = ndcg_score(targets, predictions, k=k) \n",
    " return ndcg_scores.mean() \n",
    " \n",
    "# Calculate Hit Rate@k Indicators  \n",
    "def compute_hit_rate(predictions, targets, k): \n",
    " num_hits = 0 \n",
    " for i in range(len(predictions)): \n",
    " if targets[i] in predictions[i, :k]: \n",
    " num_hits += 1 \n",
    " return num_hits /  len(predictions) \n",
    " \n",
    "# Calculate Precision@k metrics  \n",
    "def compute_precision(predictions, targets, k): \n",
    " precision_scores = [] \n",
    " for i in range(len(predictions)): \n",
    " precision = precision_score([targets[ i]], predictions[i, :k], average='micro') \n",
    "        precision_scores.append(precision) \n",
    "    return sum(precision_scores) / len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "print(\"F1-score:\", calculate_f1_score(labels, preds))\n",
    "print(\"Accuracy:\", calculate_accuracy(labels, preds))\n",
    "print(\"Precision:\", calculate_precision(labels, preds))\n",
    "print(\"Recall:\", calculate_recall(labels, preds))\n",
    "print(\"ROC_AUC:\", calculate_roc_auc(labels, preds))\n",
    "\n",
    "#print(f\"Precision@{k}: {precision:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC curves and AUC were calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC（Receiver Operating Characteristic） AUC（Area Under the ROC Curve）\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "# ROC curves and AUC \n",
    "fpr, tpr, thresholds = roc_curve(labels, preds)\n",
    "roc_auc = roc_auc_score(labels, preds) #roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Visualize the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# save\n",
    "save_dir='DataReport'\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "plt.savefig(save_dir+'BaseCnn_AUC.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"AUC:\", roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model=Net()\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test data preprocessing functions\n",
    "def load_and_preprocess_image(img_path):\n",
    "    transform_test = transforms.Compose([\n",
    "                    transforms.Resize((1024, 1024)),\n",
    "                    transforms.ToTensor()                    \n",
    "    ])\n",
    "\n",
    "    #image = glob.glob(img_path)\n",
    "    image = Image.open(img_path)   \n",
    "    image_tensor = transform_test(image)\n",
    "    \n",
    "    # Add a batch dimension to a single image\n",
    "    image_tensor=torch.unsqueeze(image_tensor, 0)  #image_tensor.unsqueeze(0)\n",
    "\n",
    "    return image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.Test a single image, load the unprocessed image directly for testing\n",
    "image_path = 'Data_Val/1.tif'  #Replace with your image path\n",
    "image = Image.open(image_path)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height = image.size\n",
    "channels = 1 if image.mode == 'L' else 3  # There is 1 channel for grayscale images and 3 channels for color images\n",
    "print(f\"Image dimensions: {width} x {height} x {channels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_and_preprocess_image(image_path).to(device)\n",
    "\n",
    "model.eval() \n",
    "output = model(image)\n",
    "#The category with the highest probability of output\n",
    "_, predicted_class = torch.max(output, dim=1)\n",
    "predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = F.softmax(output, dim=1)[0] * 100\n",
    "percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the prediction results and sort them from largest to smallest\n",
    "_, indices = torch.sort(output, descending=True)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Select a random image for the DataLoader in the preprocessed dataset for testing\n",
    "image,label = next(iter(test_dl))\n",
    "\n",
    "y_pred = model(image.to(device)  )\n",
    "y_pred = torch.argmax(y_pred, dim=1)\n",
    "\n",
    "label,y_pred.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=image[0].squeeze() #Remove the batch dimension\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = (sample.permute(1, 2, 0).numpy()+1)/2  \n",
    "plt.title(label)\n",
    "plt.imshow(sample)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Test a random image from the preprocessed dataset DataSet\n",
    "import random\n",
    "r_index = random.choice(range(len(test_ds)))\n",
    "\n",
    "x, y = test_ds[r_index]\n",
    "\n",
    "x=x.unsqueeze(0)  #x=torch.unsqueeze(x, 0)\n",
    "y = torch.as_tensor(y, dtype=torch.long)\n",
    "\n",
    "x, y = x.to(device), y.to(device)       \n",
    "\n",
    "y_pred = model(x)\n",
    "y_pred = torch.argmax(y_pred, dim=1)\n",
    "\n",
    "y.cpu().numpy(),y_pred.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
