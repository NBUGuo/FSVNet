import sys
import time
import copy 
import os
import openpyxl
from openpyxl import Workbook
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image   #  pip install pillow
import glob
import torch
from torch.utils import data
import torchvision
from torchvision import transforms
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

print('PyTorch version:', torch.__version__) 
print('Python version:',sys.version)

imgs_path = glob.glob(r'Data/*/*.tif')
imgs_path[-3:]
img_p = imgs_path[7]
img_p
img_p.split('\\')[1]
label_names = [img_p.split('\\')[1] for img_p in imgs_path]
unique_label = np.unique(label_names)
unique_label
label_to_index = dict((v, k) for k, v in enumerate(unique_label))
label_to_index
index_to_label = dict((v, k) for k, v in label_to_index.items())
index_to_label
all_labels = [label_to_index.get(la) for la in label_names]
all_labels[:5]
all_labels[-5: ]
len(imgs_path)
np.random.seed(2023)
random_index = np.random.permutation(len(imgs_path))
imgs_path = np.array(imgs_path)[random_index]
all_labels = np.array(all_labels)[random_index]
imgs_path[:5]
imgs_path[1].shape

i = int(len(imgs_path)*0.8)
train_path = imgs_path[ :i]
train_labels = all_labels[ :i]
test_path = imgs_path[i: ]
test_labels = all_labels[i: ]

# # 数据集创建
test_transform = transforms.Compose([
                    transforms.Resize((1024, 1024)),
                    transforms.CenterCrop((680,680)),
                    #transforms.RandomRotation(degrees=(40)),   
                    transforms.ToTensor(), 
                    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])# mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5]
])

train_transform = transforms.Compose([                   
                    transforms.Resize((1024, 1024)),
                    transforms.CenterCrop((680,680)),
                    #transforms.RandomRotation(degrees=(40)),  
                    transforms.ToTensor(),   
                    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])# mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5]
])


#创建Dataset的子类
class Mydataset(data.Dataset):       
    def __init__(self, img_paths, labels,transform):  #初始化 图片的路径
        self.imgs = img_paths
        self.labels = labels
        self.transforms = transform
     def __getitem__(self, index):   #实现数据切片
        img = self.imgs[index]
        label = self.labels[index]
         pil_img = Image.open(img)       
        data = self.transforms(pil_img)
         return data, label
     def __len__(self):            #返回数据总长度
        return len(self.imgs)

train_ds = Mydataset(train_path, train_labels, train_transform)
test_ds = Mydataset(test_path, test_labels, test_transform)
BATCH_SIZE =10
train_dl = data.DataLoader(
                           train_ds,
                           batch_size=BATCH_SIZE,
                           shuffle=True
                           )
test_dl = data.DataLoader(
                          test_ds,
                          batch_size=BATCH_SIZE
                          )
 imgs_batch, labels_batch = next(iter(train_dl))
 imgs_batch.shape
 imgs_batch
 labels_batch.shape
 plt.figure(figsize=(18, 12))

 for i, (img, label) in enumerate(zip(imgs_batch[-6:], labels_batch[-6:])):
    #img = img.permute(1, 2, 0).numpy()
    img = (img.permute(1, 2, 0).numpy()+1)/2    
    plt.subplot(2, 3, i+1)
    plt.title(index_to_label.get(label.item()))
    plt.imshow(img)

im=imgs_batch[0].permute(1,2,0)
im.shape
 
#im=im.numpy()
img = (im+1)/2   
type(im)
plt.imshow(im)

 # # 构建模型——利用CNN提取特征
 class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
         self.conv1 = nn.Conv2d(3, 16, 3)
        self.bn1=nn.BatchNorm2d(16)
         self.conv2 = nn.Conv2d(16, 32, 3)
        self.bn2=nn.BatchNorm2d(32)
         self.conv3 = nn.Conv2d(32, 64, 3)
        self.bn3=nn.BatchNorm2d(64)
         self.conv4 = nn.Conv2d(64, 128, 3)
        self.bn4=nn.BatchNorm2d(128)
         self.conv5 = nn.Conv2d(128,256,3)
        self.bn5=nn.BatchNorm2d(256)
         self.conv6 = nn.Conv2d(256,512,3)
        self.bn6=nn.BatchNorm2d(512)
         self.conv7 = nn.Conv2d(512,1024,3)
        self.bn7=nn.BatchNorm2d(1024)
         self.pool = nn.MaxPool2d(2, 2)
        self.drop = nn.Dropout(0.3)
         self.fc1 = nn.Linear(1024*3*3, 2048)   
        self.bn_f1=nn.BatchNorm1d(2048)  
        self.fc2 = nn.Linear(2048, 1024)
        self.bn_f2=nn.BatchNorm1d(1024) 
        self.fc3 = nn.Linear(1024, 256) 
        self.bn_f3=nn.BatchNorm1d(256) 
        self.fc4 = nn.Linear(256,128)  
        self.bn_f4=nn.BatchNorm1d(128) 
        self.fc5 = nn.Linear(128, 2)  
     def forward(self, x):
        x=self.pool(F.relu(self.conv1(x)))
        x=self.bn1(x)
        x=self.pool(F.relu(self.conv2(x)))
        x=self.bn2(x)
        x=self.pool(F.relu(self.conv3(x)))
        x=self.bn3(x)
        x=self.pool(F.relu(self.conv4(x)))
        x=self.bn4(x)  
        x=self.pool(F.relu(self.conv5(x)))
        x=self.bn5(x)
        x=self.pool(F.relu(self.conv6(x)))
        x=self.bn6(x)  
        x=self.pool(F.relu(self.conv7(x)))       
        x=self.bn7(x) 
        x=self.drop(x)        
        #print(x.size())
        x=x.view(-1, 1024*3*3)
        #x=x.view(-1, x.size(1)*x.size(2)*x.size(3))
         x=F.relu(self.fc1(x))
        x=self.bn_f1(x)        
        x=self.drop(x) 
         x=F.relu(self.fc2(x))
        x=self.bn_f2(x)
        x=self.drop(x) 
         x=F.relu(self.fc3(x))
        x=self.bn_f3(x)
        x=self.drop(x)
         x=F.relu(self.fc4(x))
        x=self.bn_f4(x)
        x=self.drop(x)
         x=self.fc5(x)
         return x
 model=Net()
 model
 # # 模型计算量和参数量
 #方法一：
from torchsummary import summary
 print(summary(model, input_size=(3, 1024, 1024)))
 #方法二：
total = sum([param.nelement() for param in model.parameters()]) 
print("Number of parameter: %.2fM" % (total/1e6))
 #方法三
from thop import profile
 input = torch.randn(1, 3,1024,1024)
flops, params = profile(model, inputs=(input,))
print('flops: ', flops, 'params: ', params)
print('计算量 flops: %.2f M, 模型的参数总量 params: %.2f M' % (flops //1e6, params //1e6))
 #model(imgs_batch)
 # # 训练模型
 #训练函数
def train_model(model, train_dataloader, criterion, optimizer, device):
     correct=0
    total=0
    running_loss = 0.0
     start_time = time.time()    
     model.train()
    for inputs, labels in train_dataloader:
        labels=torch.as_tensor(labels,dtype=torch.long)
        inputs, labels = inputs.to(device), labels.to(device)        
         predict = model(inputs)
        loss = criterion(predict,labels)
         optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        with torch.no_grad():
            predict=torch.argmax(predict,dim=1)
            correct+=(predict==labels).sum().item()
            total+=labels.size(0)
            running_loss+=loss.item()
     loss = running_loss / len(train_dataloader.dataset)
    acc = correct / total
     train_time = time.time() - start_time 
     return loss,acc
 # # 评估模型
 #评估函数
def evaluate_model(model, eval_dataloader,criterion, device):
     test_correct = 0
    test_total = 0
    test_running_loss = 0.0
     predictions = []
    targets = []
     start_time = time.time()    
     model.eval()
    with torch.no_grad():
        for inputs, labels in eval_dataloader:
            labels=torch.as_tensor(labels,dtype=torch.long)
            inputs = inputs.to(device)
             predict = model(inputs)
            loss = criterion(predict, labels)
             predict=torch.argmax(predict,dim=1)
             test_correct+=(predict==labels).sum().item()
            test_total+=labels.size(0)
            test_running_loss+=loss.item()
             predictions.extend(predict.cpu().numpy())
            targets.extend(labels.numpy())
     loss = test_running_loss / len(eval_dataloader.dataset)
    acc = test_correct / test_total
     eval_time = time.time() - start_time 
     return loss, acc, predictions, targets
 # # 开始训练和评估
 #重复训练轮次
epochs = 20
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
 # 定义损失函数和优化器  
criterion = torch.nn.CrossEntropyLoss()   #criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)
 #模型数据记录
best_model_wts = copy.deepcopy(model.state_dict())
best_acc = 0.0
 #训练数据记录
train_loss = []
train_acc = []
eval_loss = []
eval_acc = []
 #评估数据记录
eval_preds = []
eval_labels = []  
 # 训练和评估
model.to(device)
 for epoch in range(epochs):
    epoch_train_loss,epoch_train_acc = train_model(model, train_dl, criterion, optimizer, device)           
     # 测试和评估
    epoch_eval_loss,epoch_eval_acc,eval_predictions, eval_targets = evaluate_model(model, test_dl,criterion, device)
     #性能输出
    print(f"Epoch {epoch+1}/{epochs}: Train Loss: {epoch_train_loss:.4f}   Train Acc: {epoch_train_acc:.4f}  Eval_Loss： {epoch_eval_loss:.4f}   Eval_Acc:{epoch_eval_acc:.4f}")
     if epoch_eval_acc>best_acc:
        best_acc=epoch_eval_acc
        best_model_wts=copy.deepcopy(model.state_dict())
     train_loss.append(epoch_train_loss)
    train_acc.append(epoch_train_acc)
    eval_loss.append(epoch_eval_loss)
    eval_acc.append(epoch_eval_acc)        
 model.load_state_dict(best_model_wts) 
 print(f"Model_best_acc: {best_acc:.4f}")
 # # 模型参数保存
 #保存模型权重到文件
PATH='MyCNN_Weights.pth'
torch.save(model.state_dict(),PATH)
print("Model weights saved.")
 # # 模型性能绘图
 #损失绘图
plt.plot(range(1, epochs+1), train_loss, label='train_loss')
plt.plot(range(1, epochs+1), eval_loss, label='eval_loss')
plt.legend()
 # 保存图形
save_dir='数据报告'
if not os.path.isdir(save_dir):
    os.mkdir(save_dir)
 plt.savefig(save_dir+'MyCNN_Loss.png')
 #准确率绘图
plt.plot(range(1, epochs+1), train_acc, label='train_acc')
plt.plot(range(1, epochs+1), eval_acc, label='eval_acc')
plt.legend()
 # 保存图形
save_dir='数据报告'
if not os.path.isdir(save_dir):
    os.mkdir(save_dir)
 plt.savefig(save_dir+'MyCNN_Acc.png')
 #损失、准确率数据保存
save_dir='数据报告'
if not os.path.isdir(save_dir):
    os.mkdir(save_dir)
 book = openpyxl.Workbook()
sheet = book.active
sheet.title = "数据报告"
 sheet.append(['epochs', 'train_loss','test_loss','train_acc','test_acc'])
 for i in range(0, epochs):
    sheet.append([i+1, train_loss[i], eval_loss[i],train_acc[i],eval_acc[i]])
 book.save(save_dir+"/MyCNN_Loss_Accuracy.xlsx")
 #range(0,epochs).to_excel(save_dir+"/ResNet_Loss_Accuracy.xlsx", encoding="utf_8_sig")
#train_loss.to_excel(save_dir+"/ResNet_Loss_Accuracy.xlsx", encoding="utf_8_sig")
#test_loss.to_excel(save_dir+"/ResNet_Loss_Accuracy.xlsx", encoding="utf_8_sig")
#test_loss.to_excel(save_dir+"/ResNet_Loss_Accuracy.xlsx", encoding="utf_8_sig")
#test_acc.to_excel(save_dir+"/ResNet_Loss_Accuracy.xlsx", encoding="utf_8_sig")
 # # 模型参数导入
 # 实例化模型
#model=Net()
# 加载模型权重
#model.load_state_dict(torch.load(PATH))
print("Model weights loaded.")
 # # 验证模型
 # 验证模型函数
def validate_model(model, val_loader):
    model.eval()
    model.to(device)
    correct = 0
    total = 0
    all_preds = []
    all_labels = []
    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs, labels = inputs.to(device), labels.to(device)
             outputs = model(inputs)
            _, predicted = torch.max(outputs, dim=1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
             all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
     print(f'Validation Accuracy: {100 * correct / total:.2f}%')
     save_dir='数据报告'
    if not os.path.isdir(save_dir):
        os.mkdir(save_dir)
     book = openpyxl.Workbook()
    sheet = book.active
    sheet.title = "数据报告"
     sheet.append(['ground truth', 'preds'])
     for i in range(0, len(all_preds)):
        sheet.append([all_labels[i],all_preds[i]])
     book.save(save_dir+"/MyCNN_labels_preds.xlsx")
     return  all_labels,all_preds
 labels,preds=validate_model(model, test_dl)
preds
 # # 计算混淆矩阵
 #生成混淆矩阵
cm = confusion_matrix(labels, preds)
cm
 # # 绘制混淆矩阵
 # 计算混淆矩阵
from sklearn.metrics import confusion_matrix
# confusion_matrix 需要的参数：y_true(真实标签),y_pred(预测标签),normalize(归一化,'true', 'pred', 'all')
# cm = confusion_matrix(y_true=y_test, y_pred=y_pred, normalize='true')
 def plot_confusion_matrix(cm,labels,preds): 
     # 绘制混淆矩阵
    class_names = ['positive','negative']
    title="confusion_matrix"
    ylabel='True Label'
    xlabel='Predicted Label'
     tick_marks = np.arange(len(class_names))
     # 通过rc参数修改字体为Times New Roman，就可以显示中文
    plt.rcParams['font.family'] = 'Times New Roman' 
    plt.rcParams['font.size'] = 12 
    #plt.rcParams['font.weight'] = 'bold' 
    # 设置西文字体为新罗马字体
    # from matplotlib import rcParams
    # config = {
    #     "font.family":'Times New Roman',  # 设置字体类型
    #     "font.size": 80,
    #     "mathtext.fontset":'stix',
    # }
    # rcParams.update(config)
     # 通过rc参数修改字符显示，就可以正常显示负号
    #plt.rcParams['axes.unicode_minus'] = False
     plt.figure(figsize=(3.15,2.36))  # plt.figure(figsize=(18, 12)) #其中figsize=(x,y),x,y表示宽度和高度,单位英寸
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)  #plt.imshow(cm, interpolation='nearest',cmap='RdBu')
    #plt.colorbar(label='tota1 test sample number')    
     #plt.title(title)
    plt.xticks(tick_marks,class_names,rotation=45,fontsize=10)
    plt.yticks(tick_marks,class_names,fontsize=10)
    plt.ylabel(ylabel,fontsize=12)
    plt.xlabel(xlabel,fontsize=12)
     # 保存图形
    save_dir='数据报告'
    if not os.path.isdir(save_dir):
        os.mkdir(save_dir)
     plt.savefig(save_dir+'/MyCNN_Confusion_Matrix.png')
     plt.show()
 plot_confusion_matrix(cm,labels,preds)
 # 使用sklearn的ConfusionMatrixDisplay画混淆矩阵
# ConfusionMatrixDisplay 需要的参数: confusion_matrix(混淆矩阵), display_labels(标签名称列表)

 from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
 cm = confusion_matrix(labels,preds)
class_names = ['positive','negative']
  # 通过rc参数修改字体为Times New Roman，就可以显示中文
#plt.figure(figsize=(18, 12),dpi=600) #  plt.figure(figsize=(18, 12)) #其中figsize=(x,y),x,y表示宽度和高度,单位英寸
fig, ax = plt.subplots(figsize=(18, 12),dpi=600)
plt.rcParams['font.family'] = 'Times New Roman' 
plt.rcParams['font.size'] = 12 
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
 disp.plot(cmap=plt.cm.Blues)  #disp.plot(cmap=plt.cm.Blues,ax=ax),disp.plot() ,disp.plot(cmap=plt.cm.Blues) ，disp.plot(cmap=plt.cm.Blues, xticks_rotation=45)
 # 保存图形
save_dir='数据报告'
if not os.path.isdir(save_dir):
    os.mkdir(save_dir)
 plt.savefig(save_dir+'/MyCNN_Confusion_Matrix_2.png')
 plt.show()
 # 使用 seaborn 对 confusion_matrix 进行可视化
from sklearn.metrics import confusion_matrix
import seaborn as sns
 class_names = ['positive','negative']
title="confusion_matrix"
ylabel='True Label'
xlabel='Predicted Label'
 tick_marks = np.arange(len(class_names))
 cm=confusion_matrix(labels, preds)
 plt.figure(figsize=(18, 12),dpi=600)
 #sns.heatmap(cm, annot=True) 
sns.set(font_scale=1.5)   # 将混淆矩阵中的数字字体变大**
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=True,cbar_kws={'label': 'Accuracy %'}) 
 plt.xlabel(xlabel,fontsize=20, color='k') #x轴label的文本和字体大小plt.xlabel(xlabel,fontsize=20, color='k')
plt.ylabel(ylabel,fontsize=20, color='k') #y轴label的文本和字体大小plt.ylabel(ylabel,fontsize=20, color='k')
plt.xticks(tick_marks,class_names) #plt.xticks(fontsize=20) #x轴刻度的字体大小
plt.yticks(tick_marks,class_names)  #plt.yticks(fontsize=20) #y轴刻度的字体大小
#plt.title(title,fontsize=20) #图片标题文本和字体大小
 # 保存图形
save_dir='数据报告'
if not os.path.isdir(save_dir):
    os.mkdir(save_dir)
 plt.savefig(save_dir+'/MyCNN_Confusion_Matrix_4.png')
 plt.show()
 #ax = sns.heatmap(cm)
#ax=sns.heatmap(cm, vmin=60, vmax=100, xticklabels=[4,8,16,32,64,128],yticklabels=[2,4,6,8],cmap="PuBu",linewidths=.0, annot=True,cbar_kws={'label': 'Accuracy %'})
 plt.figure(figsize=(18, 12),dpi=600)
ax=sns.heatmap(cm, cmap="PuBu",linewidths=.0, annot=True,cbar_kws={'label': 'Accuracy %'})
ax.figure.axes[-1].set_ylabel('Accuracy %', size=20)
 #sns.heatmap(cm, annot=True,cmap='PuOr') 
#sns.heatmap(cm, cmap="YlGnBu",annot=True)
 plt.xlabel(xlabel,fontsize=20, color='k') #x轴label的文本和字体大小plt.xlabel(xlabel,fontsize=20, color='k')
plt.ylabel(ylabel,fontsize=20, color='k') #y轴label的文本和字体大小plt.ylabel(ylabel,fontsize=20, color='k')
plt.xticks(tick_marks,class_names,fontsize=20)# plt.xticks(tick_marks,class_names,rotation=45) #plt.xticks(fontsize=20) #x轴刻度的字体大小
plt.yticks(tick_marks,class_names,fontsize=20)  #plt.yticks(fontsize=20) #y轴刻度的字体大小
#plt.title(title,fontsize=20) #图片标题文本和字体大小
 # 保存图形
save_dir='数据报告'
if not os.path.isdir(save_dir):
    os.mkdir(save_dir)
 plt.savefig(save_dir+'/MyCNN_Confusion_Matrix_3.png')
 plt.show()

 # # 模型性能综合评估
 # 计算评价指标
from sklearn.metrics import f1_score, mean_absolute_error, mean_squared_error, roc_auc_score 
from sklearn.metrics import accuracy_score, precision_score, recall_score, ndcg_score
 # 定义计算F1-score的函数
def calculate_f1_score(true_labels, predicted_labels):
    return f1_score(true_labels, predicted_labels)
 # 定义计算MAE的函数
def calculate_mae(true_values, predicted_values):
    return mean_absolute_error(true_values, predicted_values)
 # 定义计算RMSE的函数
def calculate_rmse(true_values, predicted_values):
    return np.sqrt(mean_squared_error(true_values, predicted_values))
 # 定义计算Accuracy的函数
def calculate_accuracy(true_labels, predicted_labels):
    return accuracy_score(true_labels, predicted_labels)
 # 计算Precision@k指标
def calculate_precision(true_labels, predicted_labels):    
    return precision_score(true_labels, predicted_labels)
 # 定义计算Recall的函数
def calculate_recall(true_labels, predicted_labels):
    return recall_score(true_labels, predicted_labels)
 # 定义计算ROC AUC的函数
def calculate_roc_auc(true_labels, predicted_scores):
    return roc_auc_score(true_labels, predicted_scores)
 # 计算NDCG@k指标
def compute_ndcg(predictions, targets, k):
    ndcg_scores = ndcg_score(targets, predictions, k=k)
    return ndcg_scores.mean()
 # 计算Hit Rate@k指标
def compute_hit_rate(predictions, targets, k):
    num_hits = 0
    for i in range(len(predictions)):
        if targets[i] in predictions[i, :k]:
            num_hits += 1
    return num_hits / len(predictions)
 # 计算Precision@k指标
def compute_precision(predictions, targets, k):
    precision_scores = []
    for i in range(len(predictions)):
        precision = precision_score([targets[i]], predictions[i, :k], average='micro')
        precision_scores.append(precision)
    return sum(precision_scores) / len(predictions)
 # 测试评价计算
print("F1-score:", calculate_f1_score(labels, preds))
print("Accuracy:", calculate_accuracy(labels, preds))
print("Precision:", calculate_precision(labels, preds))
print("Recall:", calculate_recall(labels, preds))
print("ROC_AUC:", calculate_roc_auc(labels, preds))
 #print(f"Precision@{k}: {precision:.4f}")
 # # 计算ROC曲线和AUC
 #ROC（Receiver Operating Characteristic）曲线和AUC（Area Under the ROC Curve）是用于评估二分类模型性能的常用指标。
# ROC展示了不同阈值下的真阳性率（True Positive Rate）与假阳性率（False Positive Rate）之间的关系。
# AUC表示ROC曲线下的面积。AUC的值范围在0到1之间，数值越大表示模型性能越好。
 from sklearn.metrics import roc_curve, auc, roc_auc_score
 # 计算ROC曲线和AUC
fpr, tpr, thresholds = roc_curve(labels, preds)
auc=auc(fpr, tpr)
roc_auc = roc_auc_score(labels, preds) #roc_auc = auc(fpr, tpr)
 #Roc、AUC数据保存
save_dir='数据报告'
if not os.path.isdir(save_dir):
    os.mkdir(save_dir)    
book = openpyxl.Workbook()
sheet = book.active
sheet.title = "数据报告"
sheet.append(['fpr', 'tpr','thresholds','auc','roc_auc'])
for i in range(0, fpr.size):
    sheet.append([fpr[i], tpr[i],thresholds[i],auc,roc_auc])    
book.save(save_dir+"/MyCNN_Roc_Auc.xlsx")

 # 可视化ROC曲线
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
 # 保存图形
save_dir='数据报告'
if not os.path.isdir(save_dir):
    os.mkdir(save_dir)
 plt.savefig(save_dir+'/MyCNN_AUC.png')
 plt.show()
 print("AUC:", roc_auc)
 #在这个示例中，我们首先生成随机的真实标签和预测概率。然后，使用roc_curve函数计算ROC曲线的真阳性率（True Positive Rate）
#和假阳性率（False Positive Rate），以及阈值。
#使用roc_auc_score或者auc函数计算AUC值。
#最后，使用matplotlib库绘制ROC曲线图，并输出计算得到的AUC值。
#您可以根据实际的模型预测结果和真实标签替换示例代码中的y_true和y_pred，以计算和可视化您的模型的ROC和AUC指标。
 # # 定义测试模型
 #Test_model=Net()
#Test_model.load_state_dict(torch.load(PATH))
 # 测试单张图片
image_path = 'Data_Val/0.tif'  #替换为你的图片路径
image = Image.open(image_path)
plt.imshow(img)
 width, height = image.size
channels = 1 if image.mode == 'L' else 3  # 灰度图像有1个通道，彩色图像有3个通道
print(f"Image dimensions: {width} x {height} x {channels}")
 # 2、直接加载未处理的图片进行测试
#单张测试图片并进行预处理
def load_and_preprocess_image(image_path):
    transform_valid = transforms.Compose([
                    transforms.Resize((1024, 1024)),
                    transforms.ToTensor()                    
    ])
     #image = glob.glob(image_path)
    image = Image.open(image_path)   
    image_tensor = transform_valid(image)
    # 将单张图像添加到批次维度
    image_tensor=torch.unsqueeze(image_tensor, 0)#或者image_tensor.unsqueeze(0)
     return image_tensor
 image_path = 'Data_Val/0.tif'  #替换为你的图片路径
image = load_and_preprocess_image(image_path).to(device)
 output = model(image)
 #输出概率最大的类别
_, predicted_class = torch.max(output, dim=1)
predicted_class
 percentage = F.softmax(output, dim=1)[0] * 100
percentage
 # 得到预测结果，并且从大到小排序
_, indices = torch.sort(output, descending=True)
indices

 
